{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import openai\n",
    "import os\n",
    "import sklearn.metrics\n",
    "import random\n",
    "import time\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxx\"\n",
    "openai.api_key = \"sk-\"\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_utils import TASKS, load_data, load_prompt, generate_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchat_models\u001b[39;00m \u001b[39mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mschema\u001b[39;00m \u001b[39mimport\u001b[39;00m HumanMessage\n\u001b[0;32m----> 7\u001b[0m GPT_TURBO \u001b[39m=\u001b[39m ChatOpenAI(model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m, temperature\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m, max_tokens\u001b[39m=\u001b[39;49m\u001b[39m600\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_llm_openai\u001b[39m(prompt):\n\u001b[1;32m     10\u001b[0m     output \u001b[39m=\u001b[39m GPT_TURBO([HumanMessage(content\u001b[39m=\u001b[39mprompt)])\n",
      "File \u001b[0;32m~/Documents/gpt/llmlaw/.venv/lib/python3.8/site-packages/langchain/load/serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/Documents/gpt/llmlaw/.venv/lib/python3.8/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "## your LLM stack goes here\n",
    "\n",
    "\n",
    "# example of inference function for penai models\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "GPT_TURBO = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.5, max_tokens=600)\n",
    "\n",
    "def call_llm_openai(prompt):\n",
    "    output = GPT_TURBO([HumanMessage(content=prompt)])\n",
    "    return output.content\n",
    "\n",
    "\n",
    "# example of inference function for modal hosted models\n",
    "import requests\n",
    "\n",
    "def call_llm_modal(prompt):\n",
    "    r = requests.post('https://xxxxxxxxx.modal.run', json={'question': prompt})\n",
    "    output_dict = r.json()\n",
    "    output = output_dict['output']\n",
    "    completion = output[len(prompt):].strip()\n",
    "    return completion\n",
    "\n",
    "\n",
    "# example of inference function for baseten hosted models\n",
    "import baseten\n",
    "MODEL = baseten.deployed_model_version_id(\"ZBMDm4q\")\n",
    "\n",
    "def call_llm_baseten(prompt):\n",
    "    output = MODEL.predict({\"prompt\": prompt, \"do_sample\": True, \"max_new_tokens\": 300})\n",
    "    completion = output['data']['generated_text'][len(prompt):].strip()\n",
    "    return completion\n",
    "\n",
    "\n",
    "# submission for finetuned openai model\n",
    "import openai\n",
    "\n",
    "def call_llm_finetuned_openai(prompt):\n",
    "    try:\n",
    "        output = openai.Completion.create(\n",
    "                    model=\"davinci:ft-personal-2023-06-27-15-35-52\",\n",
    "                    prompt=prompt,\n",
    "                    max_tokens=5,\n",
    "                    temperature=0.0,\n",
    "                    top_p=1.0,\n",
    "                    frequency_penalty=0.5,\n",
    "    )\n",
    "        return output.choices[0].text\n",
    "    except:\n",
    "        print(\"Token is too long.\")\n",
    "        return \"\"\n",
    "    \n",
    "\n",
    "\n",
    "# submission for open source finetuned baseten model\n",
    "from baseten.models import FlanT5\n",
    "# baseten.login(\"yiVOous9.2mjkPJSPdya6FUFbGKmXaLyUF6ZxTQYs\")\n",
    "\n",
    "model = FlanT5(model_id=\"ZBMDm4q\")\n",
    "\n",
    "gen_kwargs = {\n",
    "  \"temperature\": 0,\n",
    "  \"repetition_penalty\": 1.2,\n",
    "}\n",
    "\n",
    "def call_llm_finetuned_baseten(prompt):\n",
    "    output =  model(\n",
    "        prompt,\n",
    "        max_new_tokens=512,\n",
    "        early_stopping=True,\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    return output[0]\n",
    "\n",
    "### Config below which LLM to Use\n",
    "\n",
    "call_llm = call_llm_finetuned_openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tasks: list, tasks_dir: str):\n",
    "    report = dict()\n",
    "    for task in tasks:\n",
    "        train_df, test_df = load_data(task=task, tasks_dir=tasks_dir)\n",
    "        prompt_template = load_prompt(prompt_name=\"base_prompt.txt\", task=task, tasks_dir=tasks_dir)\n",
    "        prompts = generate_prompts(prompt_template=prompt_template, data_df=train_df)\n",
    "        report[task] = dict()\n",
    "        targets = list()\n",
    "        outputs = list()\n",
    "        print('task', task)\n",
    "        for prompt, data in zip(prompts, train_df.iterrows()):\n",
    "            datapoint_id, data = data\n",
    "            output = call_llm(prompt)\n",
    "            output = output.strip()\n",
    "            targets.append(data['answer'])\n",
    "            outputs.append(output)\n",
    "            success = output == data['answer']\n",
    "            report[task][datapoint_id] = {\n",
    "                'prompt': prompt,\n",
    "                'generated_output': output,\n",
    "                'correct_output': data['answer'],\n",
    "                'success': output == data['answer']\n",
    "            }\n",
    "        report[task]['balanced_accuracy'] = sklearn.metrics.balanced_accuracy_score(targets, outputs)\n",
    "        print('task balanced accuracy:', report[task]['balanced_accuracy'])\n",
    "        print()\n",
    "    \n",
    "    print('Total Balanced Accuracy:', sum([report[task]['balanced_accuracy'] if not math.isnan(report[task]['balanced_accuracy']) else 0 for task in tasks])/len(tasks))\n",
    "    \n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task maud_change_in_law:__subject_to_\"disproportionate_impact\"_modifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:openai:error_code=None error_message=\"This model's maximum context length is 2049 tokens, however you requested 2491 tokens (2486 in your prompt; 5 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task balanced accuracy: 0.0\n",
      "\n",
      "task maud_fiduciary_exception:__board_determination_standard\n",
      "task balanced accuracy: 0.0\n",
      "\n",
      "task contract_nli_notice_on_compelled_disclosure\n",
      "task balanced accuracy: 0.0\n",
      "\n",
      "task diversity_6\n",
      "task balanced accuracy: 0.0\n",
      "\n",
      "task opp115_third_party_sharing_collection\n",
      "task balanced accuracy: 0.0\n",
      "\n",
      "task opp115_data_retention\n",
      "task balanced accuracy: 0.0\n",
      "\n",
      "task maud_cor_standard_(superior_offer)\n",
      "task balanced accuracy: 0.0\n",
      "\n",
      "task learned_hands_crime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:openai:error_code=None error_message=\"This model's maximum context length is 2049 tokens, however you requested 2091 tokens (2086 in your prompt; 5 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task balanced accuracy: 0.0\n",
      "\n",
      "task maud_tail_period_length\n",
      "task balanced accuracy: 0.0\n",
      "\n",
      "task maud_\"financial_point_of_view\"_is_the_sole_consideration\n",
      "task balanced accuracy: 0.0\n",
      "\n",
      "task supply_chain_disclosure_best_practice_certification\n",
      "task balanced accuracy: 0.0\n",
      "\n",
      "task cuad_uncapped_liability\n",
      "task balanced accuracy: 0.0\n",
      "\n",
      "task opp115_policy_change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task balanced accuracy: 0.0\n",
      "\n",
      "task cuad_irrevocable_or_perpetual_license\n"
     ]
    }
   ],
   "source": [
    "tasks_dir = '../legalbench'\n",
    "\n",
    "# warnings are to be expected\n",
    "\n",
    "random.seed(1)\n",
    "report = evaluate(tasks=random.sample(TASKS, 5), tasks_dir=tasks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legalbencheval",
   "language": "python",
   "name": "legalbencheval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
